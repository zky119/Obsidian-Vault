逻辑回归（Logistic Regression）是一种用于解决二分类问题的线性模型，它的基本思想是通过对线性函数的结果进行sigmoid变换，将其转换为介于0和1之间的概率值。我们来推导逻辑回归的公式。

首先，线性函数可以表示为：

$$
z = w^Tx + b
$$

其中，$w$ 是特征的权重，$x$ 是特征向量，$b$ 是偏置项。

接下来，我们引入sigmoid函数，将线性函数的结果映射到[0, 1]区间。

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

逻辑回归模型的预测值为：

$$
\hat{y} = \sigma(w^Tx + b)
$$

我们的目标是，通过训练数据来学习权重 $w$ 和偏置项 $b$，使得预测值 $\hat{y}$ 尽可能接近真实标签 $y$。

我们使用最大似然估计法来推导损失函数。给定样本 $(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)$，我们考虑似然函数：

$$
L(w, b) = \prod_{i=1}^n P(Y=y_i | X=x_i; w, b)
$$

由于 $y_i$ 只有0和1两个取值，我们可以将 $P(Y=y_i | X=x_i; w, b)$ 表示为：

$$
P(Y=y_i | X=x_i; w, b) = \hat{y_i}^{y_i} (1 - \hat{y_i})^{(1 - y_i)}
$$

代入似然函数，得到：

$$
L(w, b) = \prod_{i=1}^n [\hat{y_i}^{y_i} (1 - \hat{y_i})^{(1 - y_i)}]
$$

为了方便求解，我们取对数似然函数：

$$
\ell(w, b) = \log L(w, b) = \sum_{i=1}^n [y_i \log \hat{y_i} + (1 - y_i) \log (1 - \hat{y_i})]
$$

我们的目标是求解最大化对数似然函数的参数 $w$ 和 $b$。为了方便优化，我们可以取负对数似然函数，将最大化问题转化为最小化问题。这就得到了逻辑回归的损失函数（也称为交叉熵损失函数）：

$$
J(w, b) = -\ell(w, b) = -\sum_{i=1}^n [y_i \log \hat{y_i} + (1 - y_i) \log (1 - \hat{y_i})]
$$

我们可以通过梯度下降法来最小化损失函数，求解逻辑回归的参数 $w$ 和 $b$。