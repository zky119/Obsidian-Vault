

交叉验证（Cross-validation）是一种评估机器学习模型性能的方法，它通过将数据集划分为多个子集来进行模型的训练和验证。交叉验证可以更准确地评估模型在未见数据上的表现，并帮助选择合适的模型参数。

常见的交叉验证方法有：

1. 简单交叉验证（Simple Cross-Validation）：
   简单交叉验证是将数据集随机划分为两个互斥的子集，一个用于训练模型，一个用于验证模型。通常将数据集划分为70%的训练集和30%的验证集。
   
2. K折交叉验证（K-Fold Cross-Validation）：
   K折交叉验证将数据集均匀地划分为K个互斥的子集，其中K-1个子集用于训练模型，剩下的1个子集用于验证模型。这个过程会重复K次，每次使用不同的验证集。最后将K次验证结果的平均值作为模型的性能评估指标。
![[Pasted image 20230522140533.png]]

```python
from sklearn.model_selection import cross_val_score

# 使用交叉验证来评估模型，计算分数，返回每次在验证集上的预测结果分数。 
# estimator：要评估的模型。 
# X，y：样本与对应的标签。 
# cv：交叉验证的折数，默认为5折。 # scoring：评分的计算标准。 
score = cross_val_score(lr, X_train_scale, y_train, cv=5, scoring="f1")
```

3. 留一交叉验证（Leave-One-Out Cross-Validation）：
   留一交叉验证是K折交叉验证的一种特殊情况，其中K等于数据集的样本数。即每次使用单个样本作为验证集，其余样本作为训练集。这种方法适用于样本量较小的情况。

4. 分层K折交叉验证（Stratified K-Fold Cross-Validation）：
   分层K折交叉验证在划分数据集时保持样本类别的比例，确保每个子集中的类别分布与整个数据集中的类别分布相似。这种方法适用于类别不平衡的数据集。

交叉验证的步骤如下：
1. 将数据集划分为K个互斥的子集。
2. 对于每个子集，将其作为验证集，其他子集作为训练集。
3. 在每个子集上训练模型，并在验证集上评估模型性能。
4. 计算K次验证结果的平均值作为模型的性能评估指标。

交叉验证能够更全面地评估模型的性能，并减少因数据集划分方式不同而引起的偶然性。它在模型选择、参数调优和比较不同模型之间的性能时非常有用。