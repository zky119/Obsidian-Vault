# 基础代码
```python
tree = DecisionTreeClassifier(criterion="gini")
tree = tree.fit(X_train, y_train)
print("决策树分类正确率：")
print(tree.score(X_train, y_train))
print(tree.score(X_test, y_test))

# n_estimators：基本评估器（决策树）的数量。
# max_samples：每次抽样用于训练基本评估器的样本数量。
# bootstrap：如果为True（默认值），抽样允许重复（放回抽样）。如果为False，则使用所有的样本训练每棵决策树。
# 注意该参数的含义与bagging的参数有所不同。
# max_samples：当bootstrap为True时，训练每棵决策树所使用的样本数。默认为None（使用所有的样本）。
rf = RandomForestClassifier(
    n_estimators=100,
    criterion="gini",
    random_state=0,
    bootstrap=True,
    max_samples=0.9,
    max_features="sqrt"
    )
rf.fit(X_train, y_train)
print("随机森林正确率：")
print(rf.score(X_train, y_train))
print(rf.score(X_test, y_test))
  

et = ExtraTreesClassifier(
    n_estimators=100,
    criterion="gini",
    random_state=0,
    bootstrap=True,
    max_samples=0.9,
    max_features="sqrt"
    )
et.fit(X_train, y_train)
print("极度随机树正确率：")
print(et.score(X_train, y_train))
print(et.score(X_test, y_test))
```

# 特征重要度

```python
from sklearn.datasets import load_digits
digits = load_digits()
X, y = digits.data, digits.target
# 只取0与6的图像。
mask = (y == 0) | (y == 6)
X = X[mask]
y = y[mask]
# 图像为8 * 8。保存在长度为64的数组中。
print(X.shape, y.shape)
row, col = 5, 4
fig, ax = plt.subplots(row, col)
ax = ax.ravel()
fig.set_size_inches(15, row * 5)
for i in range(row * col):
    ax[i].imshow(X[i].reshape(8, 8), cmap="gray")
```

```python
rf = RandomForestClassifier(n_estimators=100, max_features=0.8,
random_state=0, bootstrap=True, n_jobs=-1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=80)
rf.fit(X_train, y_train)
print(rf.score(X_train, y_train))
print(rf.score(X_test, y_test))
```

```python
# 返回特征的重要度。特征的重要度根据该特征对目标变量（y）的可预测性来度量。
# 特征对目标变量的可预测性越有帮助，则重要度越大，否则越小。
importances = rf.feature_importances_
print(importances)
# 特征重要度的权重之和为1。
print(np.sum(importances))
importances = importances.reshape(8, 8)
# 绘制特征的重要度。
plt.matshow(importances, cmap=plt.cm.hot)
plt.colorbar()
```