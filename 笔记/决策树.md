预测的方式为： 
- 对于分类树，使用叶子节点中，<font color="#f47983">数量最多</font>的<font color="#f47983">类别</font>，作为未知样本的类别。 
- 对于回归树，使用叶子节点中，所有样本标签（ $y$）<font color="#f47983">均值</font>，作为未知样本的输出值（$\hat{y}$ ）。

# 信息熵
- 信息熵，在1948年由香农提出。用来衡量信息的不确定性或随机性。<font color="#f47983">不确定性越大，则信息熵越大，反之，信息熵越小。</font>


- $$\left\{\begin{array}{c}P(X=V_1)=p_1\\ P(X=V_2)=p_2\\ P(X=V_3)=p_3\\ \cdots\\ P(X=V_m)=p_m\end{array}\right.$$
- $$p_1 + p_2 + \cdots + p_m = 1$$
- $$\begin{aligned}
&H(X)=-p_1*log_2p_1-p_2*log_2p_2-\cdots-p_m*log_2p_m \\
&=-\sum_{i=1}^m p_ilog_2p_i
\end{aligned}$$

![[Pasted image 20230618152350.png]]

# 信息增益
- $$IG(D_p,f)=I(D_p)-\sum_{j=1}^n\frac{N_j}{N_p}I(D_j)$$
- 由于是二叉树，所以
- $$IG(D_p,f)=I(D_p)-\frac{N_{limit}}{N_p}I(D_{left})-\frac{N_{right}}{N_p}I(D_{right})$$

# 训练规则

在sklearn中，训练分类决策树的具体规则如下：

- 所有特征均为数值类型。
	- 对于类别变量，可以转换为离散值。
	- 每个特征，会设定一个阈值，小于等于阈值，划分到左子树，否则划分到右子树。
- 从根节点开始，选择可获得最大信息增益的特征进行分裂（实现信息增益最大化）。
- 对子节点继续选择能够获得最大信息增益的特征进行分裂，直到满足如下条件之一，停止分裂。
	- 所有叶子节点中的样本属于同一个类别
	- 树达到指定的最大深度（max_depth），每次分裂视为一层。
	- 节点包含的样本数量小于指定的最小分裂样本数量（min_samples_split）。
	- 如果节点分裂后，叶子节点包含的样本数量小于指定的叶子最小样本数量（min_samples_leaf）。

# 不纯度度量标准

不纯度可以采用如下方式度量：
- 信息熵（Entropy）$$I_H(D)=-\sum_{i=1}^mp(i\mid D)log_2p(i\mid D)$$
- 基尼系数（Gini Index）$$I_G(D)=1-\sum_{i=1}^m p(i\mid D)^2$$
- 错误率（classification error）$$I_E(D)=1-max\{p(i\mid D)\}$$
# 决策树算法

- <font color="#f47983">ID3</font> 算法是非常经典的决策树算法，该算法描述如下：
	- 使用多叉树结构。
	- 使用信息熵作为不纯度度量标准，选择信息增益最大（信息熵最小）的特征分割数据。
	- 缺点
		- 不支持连续特征
		- 持缺失值。
		- 仅支持分类，不支持回归。
		- 在选择特征时，会倾向于选择类别多的特征。
- <font color="#f47983">C4.5</font> 是在ID3算法上改进而来，该算法描述如下：
	- 使用多叉树结构。
	- 仅支持分类，不支持回归。
	- 支持对缺失值的处理。
	- 支持将连续值进行离散化处理。
	- 使用信息熵作为不纯度度量标准，但选择信息增益率（而不是信息增益）最大的特征分裂节点。
	- 信息增益率的定义方式为$$I G_{R a t i o}(D_{p},f)=\frac{I G_{H}(D_{p},f)}{I_{H}(f)}$$
	- $I_{H}(f)$： 根据特征 的不同类别值比例（概率），计算得到的信息熵。
	- 之所以从信息增益改为信息增益率，是因为在ID3算法中，倾向于选择类别多的特征，因此，经过这样的调整，在C4.5中就可以得到缓解。因为类别多的特征在计算信息熵$I_{H}(f)$ 时，往往会比类别少的特征信息熵大。这样，就可以在分母上进行一定的惩罚。
- <font color="#f47983">CART </font>（Classification And Regression Tree），分类与回归树。该算法描述如下：
	- 使用二叉树结构。
	- 支持连续值与缺失值处理。
	- 既支持分类，也支持回归。
		- 使用基尼系数作为不纯度度量标准，选择基尼增益最大的特征分裂节点。（分类）
		- 使用[[MSE]]或[[MAE]]最小的特征分类节点。（回归）

# 回归决策树
- 回归决策树在选择特征上，每次选择能够使得MSE或MAE最小的特 征，用来分裂节点。

# 程序实现

## [[分类决策树]]
## [[回归决策树]]

```python
print(tree.feature_importances_)
```