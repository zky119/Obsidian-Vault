
|               | GBDT                                                                 | XGBoost                                                          |
| ------------- | -------------------------------------------------------------------- | ---------------------------------------------------------------- |
| 分类器        | 决策树                                                               | 决策树，线性分类器                                               |
| 使用导数      | 一阶导数：前一个弱分类器的残差（一阶导数），作为后一个弱分类器的输入 | 一阶导数和二阶导数：弱分类器的输入除了残差之外，还包含了二阶导数 |
| 正则项        | 无                                                                   | L1,L2                                                            |
| Shrinkage策略 |                                                                      |                                                                  |
| 列抽样        | feature是全部（M）特征                                                       | feature是m个，m << M                                             |
| 处理缺失值    | 不能处理缺失值                                                       | 可以自动学习出分裂方向                                           |
| 并行处理      | 不支持：每一次迭代都要对特征值进行排序                               | 支持：训练前会将特征进行排序，保存为block结构 特征层面的并行     |
|               |                                                                      |                                                                  |


# 1， [[XGBoost和GBDT的区别]]

# 2，目标函数
XGBoost算法的目标（优化目标）函数由两部分构成： 
1. 损失函数 
2. 正则项
$$\text{Obj}=\sum_{i=1}^n l(y_i,\hat{y}_i)+\sum_{k=1}^K\Omega(f_k)$$
$n$：样本数量。 
$y_i$：第 $i$个样本的真实值。 
$\hat{y_i}$：第$i$ 个样本的预测值。 
$l(y_i,\hat{y}_i)$：损失函数，计算第 $i$个样本的损失。 
$K$：集成的基本模型数量。 
$f_k$：第 $k$个基本模型。 
$\Omega(f_k)$：正则化项，用来惩罚模型 $f_k$的复杂性。

## 损失函数
- 当执行回归任务时，损失函数通常为平方和损失函数：
$$\sum_{i=1}^n l(y_i,\hat{y}_i)=\sum_{i=1}^n(y_i-\hat{y}_i)^2$$
- 当执行分类任务时，损失函数通常为对数损失函数：
$$\sum_{i=1}^{n}l(y_i,\hat{y}_i)=-\sum_{i=1}^{n}[y_i logp_i+(1-y_i)ln(1-p_i)]$$

## 正则项
正则项用来体现模型的复杂程度。模型越复杂，则惩罚力度越大。 
对于决策树来说，模型的复杂度包含以下两个方面： 
1. 叶节点数量 
	1. 叶节点数量越多，意味着决策树的深度越大，模型的复杂度也就越高。 
2. 叶权值 
	1. 叶权值为叶子节点的权重，指叶子节点的输出值（叶子节点所有样本的输出值的平均值）
	2. 叶权值可以理解为该叶节点对预测结果的贡献程度。叶权值越大，说明该叶节点对预测结果的影响越大，模型的 复杂度也就越高。

- 在XGBoost中，我们将模型复杂度定义如下：
$$\Omega(f_k)=\gamma T+\frac{1}{2}\lambda\sum_{j=1}^Tw_j^2$$

- $T$：树中叶子节点的数量。 
- $\gamma$：树结构的复杂度控制参数。 
- $w$：叶权值。
- $\lambda$：叶子节点权重的正则化参数。

## 泰勒展开

- 设损失函数的一阶导数与二阶导数：
	- 一阶导数 $g_i$ 表示了当前模型对样本 $i$ 的预测误差的梯度
	  $$g_ {i}  =  \partial _ {{y_ {i}}  (t-1)}l(  y_ {i}  ,  \widehat {y}_ {i}^ {(t-1)})=l^{\prime}(y_{i},y_{i}^{(t-1)}) $$
	- 二阶导数 $h_i$​ 表示了当前模型对样本 $i$ 的预测误差的曲率。
	  $$h_ {i}  =  \partial _ {{y_ {i}}  (t-1)}^2{l(  y_ {i}  ,  \widehat {y}_ {i}^ {(t-1)})}=l^{\prime\prime}(y_{i},y_{i}^{(t-1)}) $$
- 通过结合一阶导数和二阶导数，我们可以更准确地调整叶节点权值，以使模型在下一轮迭代中更好地拟合数据。

- 泰勒展开的模板$$f(x) = f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3 + \cdots$$
- 将损失函数在 ${y}_ {i}^ {(t-1)}$ 处按照二阶泰勒展开：
$$\begin{aligned}
&\text{Obj}^{(t)}=\sum_{i=1}^n l(y_i,\hat{y}_i^{(t-1)}+f_t(x_i))+\Omega(f_t) \\
&\approx\sum_{i=1}^n[l(y_i,\hat{y}_i^{(t-1)})+g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t) \\
&\approx\sum_{i=1}^n[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)
\end{aligned}$$




