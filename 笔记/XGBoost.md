# 1， [[XGBoost和GBDT的区别]]

# 2，目标函数
XGBoost算法的目标（优化目标）函数由两部分构成： 
1. 损失函数 
2. 正则项
$$\text{Obj}=\sum_{i=1}^n l(y_i,\hat{y}_i)+\sum_{k=1}^K\Omega(f_k)$$
$n$：样本数量。 
$y_i$：第 $i$个样本的真实值。 
$\hat{y_i}$：第$i$ 个样本的预测值。 
$l(y_i,\hat{y}_i)$：损失函数，计算第 $i$个样本的损失。 
$K$：集成的基本模型数量。 
$f_k$：第 $k$个基本模型。 
$\Omega(f_k)$：正则化项，用来惩罚模型 $f_k$的复杂性。

## 损失函数
- 当执行回归任务时，损失函数通常为平方和损失函数：
$$\sum_{i=1}^n l(y_i,\hat{y}_i)=\sum_{i=1}^n(y_i-\hat{y}_i)^2$$
- 当执行分类任务时，损失函数通常为对数损失函数：
$$\sum_{i=1}^{n}l(y_i,\hat{y}_i)=-\sum_{i=1}^{n}[y_i logp_i+(1-y_i)ln(1-p_i)]$$

## 正则项
正则项用来体现模型的复杂程度。模型越复杂，则惩罚力度越大。 
对于决策树来说，模型的复杂度包含以下两个方面： 
1. 叶节点数量 
	1. 叶节点数量越多，意味着决策树的深度越大，模型的复杂度也就越高。 
2. 叶权值 
	1. 叶权值为叶子节点的权重，指叶子节点的输出值（叶子节点所有样本的输出值的平均值）
	2. 叶权值可以理解为该叶节点对预测结果的贡献程度。叶权值越大，说明该叶节点对预测结果的影响越大，模型的 复杂度也就越高。

- 在XGBoost中，我们将模型复杂度定义如下：
$$\Omega(f_k)=\gamma T+\frac{1}{2}\lambda\sum_{j=1}^Tw_j^2$$

- $T$：树中叶子节点的数量。 
- $\gamma$：树结构的复杂度控制参数。 
- $w$：叶权值。
- $\lambda$：叶子节点权重的正则化参数。

