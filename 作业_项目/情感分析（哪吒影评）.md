# 项目背景

- 是<font  color="#f47983"  size="5">文本分类</font>中常见的应用场景
- 常见的情感分析就是客户对商品或者服务的反馈，例如顾客对商品，酒店的评价等场景。
# 任务

- 根据电影评论中的内容，进行文本预处理，建模等操作， 从而可以识别评论的感情色彩，节省人力资源

# 实现

- 能够对文本数据进行预处理。 
	- 文本清洗。 
	- 分词，去除停用词。 
	- 文本向量化。 
- 能够通过Python统计词频，生成词云图。 
- 能够通过方差分析，进行特征选择。 
- 能够根据文本向量，对文本数据进行分类。

# 数据集
![[comment.csv]]

# 利用正则去除符号

```python 
import re
pattern = r"[!\"#$%&'()*+,-./:;<=>?@[\\\]^_`{|}~—！，。？、￥…（）：【】《》‘’“”\s]+"
re_obj = re.compile(pattern)
# A方案
def clear(text):
	return re.sub(pattern, "", text)
# B方案
def clear(text):
	return re_obj.sub("", text)
data["comment"] = data["comment"].apply(clear)
```

# 分词

```python
import jieba
s = "今天，外面下了一场很大的雨。"
# cut与lcut的区别：前者返回生成器，后者返回列表。
words = jieba.cut(s)
print(words)
print(list(words))
words = jieba.lcut(s)
print(words)
```

```python
# A方案
def cut_word(text):
return jieba.cut(text)
# B方案
def cut_word(text):
return jieba.lcut(text)
data["comment"] = data["comment"].apply(cut_word)
```

# 停用词处理

```python
# A方案，使用set。
def get_stopword():
    s = set()
    with open("stopword.txt", encoding="UTF-8") as f:
        for line in f:
            s.add(line.strip())
    return s
    
# B方案，使用list。
def get_stopword():
    s = list()
    with open("stopword.txt", encoding="UTF-8") as f:
        for line in f:
            s.append(line.strip())
    return s
    
def remove_stopword(words):
    return [word for word in words if word not in stopword]
    
stopword = get_stopword()
data["comment"] = data["comment"].apply(remove_stopword)
```

# 数据基本分析

## 1，用户所在城市
```python
fig, ax = plt.subplots(1, 2)
fig.set_size_inches(12, 4)
count = data["city"].value_counts()
top = count.iloc[:10]
bottom = count.iloc[-10:]
for index, d, title in zip(range(2), [top, bottom], ["前十城市", "后十城市"]):
    a = sns.barplot(x=d.index, y=d.values, ax=ax[index])
    # 旋转45度，避免字体重叠。
    a.set_xticklabels(a.get_xticklabels(), rotation=60)
    for container in a.containers:
    a.bar_label(container)
    a.set_title(title)
```

## 2，用户等级

```python
a = sns.countplot(x="level", data=data, log=True) 
for container in a.containers:
	a.bar_label(container)
```

## 3，评分

```python
a = sns.countplot(x="score", data=data, log=True) 
for container in a.containers: 
	a.bar_label(container)
```

```python
data_period = [0, 2.51, 4.01, 5] 
# 也可以使用matplotlib来绘制直方图。 
# plt.hist(data["score"], bins=data_period, log=True) 
# bins指定的区间为左闭右开，最后一个区间为双闭。 
a = sns.histplot(data["score"], bins=data_period) 
# seaborn的histplot方法没有设置log的参数，需要通过axes自行设置。 
a.set_yscale("log") 
# 让x轴显示的数值与bins指定的区间一致。 
a.set_xticks(data_period) 
for container in a.containers: 
	a.bar_label(container)
```

## 4，性别对比

```python
# 0：未知，1：男，2：女。 
a = sns.countplot(x="gender", data=data) 
for container in a.containers: 
	a.bar_label(container)
```

5，评论时间分布

```python
# 时间列的格式：2019-07-31 21:21:02
hour = data["time"].str.extract(r" (\d{2}):", expand=False)
hour = hour.astype(np.int32)
# 直方图的区间的前闭后开，最后一个区间双闭。
time_period = [0, 6, 12, 18, 24]
a = sns.histplot(hour, bins=time_period)
a.set_xticks(time_period)
for container in a.containers:
	a.bar_label(container)
```

# 词汇统计

```python
from itertools import chain
from collections import Counter
li_2d = data["comment"].tolist()
# 将二维列表扁平化为一维列表。
li_1d = list(chain.from_iterable(li_2d))
print(f"总词汇量：{len(li_1d)}")
c = Counter(li_1d)
print(f"不重复词汇数量：{len(c)}")
common = c.most_common(15)
print(common)
```

# 可视化

```python
d = dict(common)
plt.figure(figsize=(12, 5))
# seaborn内部不支持dict_keys与dict_values类型，maplotlib.bar可以。
a = sns.barplot(x=list(d.keys()), y=list(d.values()))
for container in a.containers:
    a.bar_label(container)
```

# 频率统计

```python
total = len(li_1d)
percentage = [v * 100 / total for v in d.values()]
print([f"{v:.2f}%" for v in percentage])
plt.figure(figsize=(12, 5))
a = sns.barplot(x=list(d.keys()), y=percentage)
for container in a.containers:
    a.bar_label(container, fmt="%.2f")
```

# 频数分布统计
```python
plt.figure(figsize=(12, 5))
v = list(c.values())
# 最大的词频数，对应的log值。
end = np.log10(max(v))
# 根据在10^0 ~ 10^end之间，生成等比数列，作为直方图的区间（bins）。
a = sns.histplot(v, bins=np.logspace(0, end, num=10))
for container in a.containers:
    a.bar_label(container)
a.set_xscale("log")
a.set_yscale("log")
```

# 评论词汇长度统计

```python
plt.figure(figsize=(12, 5))
# 计算每个评论的用词数。
num = [len(li) for li in li_2d]
# 统计用词最多的前15个评论。
length = 15
a = sns.barplot(x=np.arange(1, length + 1), y=sorted(num, reverse=True)[:length])
for container in a.containers:
    a.bar_label(container)
```

# 评论词汇长度分布统计

```python
plt.figure(figsize=(12, 5))
a = sns.histplot(num, bins=15)
a.set_yscale("log")
for container in a.containers:
    a.bar_label(container)
```

# 生成词云图

## 1，标准词云图

```python
from wordcloud import WordCloud
# 需要指定字体的位置，否则中文无法正常显示。
wc = WordCloud(font_path=r"C:/Windows/Fonts/STFANGSO.ttf", width=800, height=600)
# WordCloud要求传递的词汇使用空格分开的字符串。
join_words = " ".join(li_1d)
img = wc.generate(join_words)
plt.figure(figsize=(12, 10))
plt.imshow(img)
plt.axis('off')
# 将图像保存到本地。
wc.to_file("wordcloud.png")
```

# 2，自定义背景

```python
wc = WordCloud(font_path=r"C:/Windows/Fonts/STFANGSO.ttf",
mask=plt.imread("../imgs/map.jpg"))
img = wc.generate(join_words)
plt.figure(figsize=(12, 10))
plt.imshow(img)
plt.axis('off')
```

3，另类词云图
- 通过WordCloud对象的generate方法生成的词云图，词汇的大小并不是严格根据词频数量显示的。如果需要传统 意义的词云图，可以调用generate_from_frequencies方法。
```python
plt.figure(figsize=(12, 10))
img = wc.generate_from_frequencies(c)
plt.imshow(img)
plt.axis('off')
```

# [[文本向量化]]