L2正则化，也称为岭回归（Ridge Regression），是一种用于线性回归模型的正则化方法，它通过添加L2范数的惩罚项来控制模型的复杂度。

在L2正则化中，我们将线性回归的损失函数加上L2范数的惩罚项，形成新的带有正则化项的损失函数。该损失函数可以表示为：

$$
\text{Loss} = \frac{1}{2N} \sum_{i=1}^{N} (y_i - \hat{y_i})^2 + \lambda \sum_{j=1}^{p} w_j^2
\
$$

其中，
- $N$ 表示样本数量，
- $y_i$ 表示实际观测值，
- $\hat{y_i}$ 表示预测值，
- $p$ 表示特征的数量，
- $w_j$表示第 $j$个特征的权重，
- $\lambda$是正则化参数，用来控制正则化的强度。

在损失函数中，第一项表示模型的拟合误差，也就是预测值与实际观测值之间的差异。第二项是L2范数的惩罚项，它对模型的权重进行惩罚，使得权重趋向于较小的值，从而降低模型的复杂度。

通过增加L2正则化项，我们可以有效地减小模型的过拟合风险，提高模型的泛化能力。正则化参数 \(\lambda\) 控制了正则化的强度，较大的 \(\lambda\) 值会导致更强的正则化效果，使得权重更加趋向于零。

在实际应用中，我们可以使用不同的方法（如交叉验证）来选择合适的正则化参数 \(\lambda\)，以达到最佳的模型性能和泛化能力。
